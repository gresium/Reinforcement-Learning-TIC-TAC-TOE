{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e8ad148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58b0f92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tic_tac_toe_rl.py\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "BOARD_ROWS = 3\n",
    "BOARD_COLS = 3\n",
    "\n",
    "class State:\n",
    "    \"\"\"\n",
    "    Environment: Tic-Tac-Toe board.\n",
    "    Board values: 1 for Player 1 (X), -1 for Player 2 (O), 0 empty.\n",
    "    \"\"\"\n",
    "    def __init__(self, p1, p2):\n",
    "        self.board = np.zeros((BOARD_ROWS, BOARD_COLS), dtype=int)\n",
    "        self.p1 = p1\n",
    "        self.p2 = p2\n",
    "        self.is_end = False\n",
    "        self.board_hash: Optional[str] = None\n",
    "        self.player_symbol = 1  # Player 1 starts\n",
    "\n",
    "    # unique string for current board state\n",
    "    def get_hash(self) -> str:\n",
    "        self.board_hash = str(self.board.reshape(BOARD_ROWS * BOARD_COLS))\n",
    "        return self.board_hash\n",
    "\n",
    "    # check terminal and return outcome: 1 win, -1 loss, 0 tie, None not end\n",
    "    def winner(self) -> Optional[int]:\n",
    "        # rows\n",
    "        for i in range(BOARD_ROWS):\n",
    "            row_sum = np.sum(self.board[i, :])\n",
    "            if row_sum == 3:\n",
    "                self.is_end = True\n",
    "                return 1\n",
    "            if row_sum == -3:\n",
    "                self.is_end = True\n",
    "                return -1\n",
    "        # cols\n",
    "        for j in range(BOARD_COLS):\n",
    "            col_sum = np.sum(self.board[:, j])\n",
    "            if col_sum == 3:\n",
    "                self.is_end = True\n",
    "                return 1\n",
    "            if col_sum == -3:\n",
    "                self.is_end = True\n",
    "                return -1\n",
    "        # diagonals\n",
    "        diag1 = sum(self.board[i, i] for i in range(BOARD_COLS))\n",
    "        diag2 = sum(self.board[i, BOARD_COLS - 1 - i] for i in range(BOARD_COLS))\n",
    "        if diag1 == 3 or diag2 == 3:\n",
    "            self.is_end = True\n",
    "            return 1\n",
    "        if diag1 == -3 or diag2 == -3:\n",
    "            self.is_end = True\n",
    "            return -1\n",
    "\n",
    "        # tie\n",
    "        if len(self.available_positions()) == 0:\n",
    "            self.is_end = True\n",
    "            return 0\n",
    "\n",
    "        # not end\n",
    "        self.is_end = False\n",
    "        return None\n",
    "\n",
    "    def available_positions(self) -> List[Tuple[int, int]]:\n",
    "        positions = []\n",
    "        for i in range(BOARD_ROWS):\n",
    "            for j in range(BOARD_COLS):\n",
    "                if self.board[i, j] == 0:\n",
    "                    positions.append((i, j))\n",
    "        return positions\n",
    "\n",
    "    def update_state(self, position: Tuple[int, int]):\n",
    "        self.board[position] = self.player_symbol\n",
    "        # switch player\n",
    "        self.player_symbol = -1 if self.player_symbol == 1 else 1\n",
    "\n",
    "    def give_reward(self):\n",
    "        result = self.winner()\n",
    "        if result == 1:          # p1 win\n",
    "            self.p1.feed_reward(1.0)\n",
    "            self.p2.feed_reward(0.0)\n",
    "        elif result == -1:       # p2 win\n",
    "            self.p1.feed_reward(0.0)\n",
    "            self.p2.feed_reward(1.0)\n",
    "        else:                    # tie\n",
    "            self.p1.feed_reward(0.5)\n",
    "            self.p2.feed_reward(0.5)\n",
    "\n",
    "    def reset(self):\n",
    "        self.board[:] = 0\n",
    "        self.is_end = False\n",
    "        self.board_hash = None\n",
    "        self.player_symbol = 1\n",
    "\n",
    "    def show(self):\n",
    "        s = {1: \"X\", -1: \"O\", 0: \" \"}\n",
    "        print(\"\\n---------\")\n",
    "        for i in range(BOARD_ROWS):\n",
    "            row = \"|\".join(s[self.board[i, j]] for j in range(BOARD_COLS))\n",
    "            print(row)\n",
    "            if i < BOARD_ROWS - 1:\n",
    "                print(\"-----\")\n",
    "        print(\"---------\")\n",
    "\n",
    "\n",
    "class Player:\n",
    "    def __init__(self, name: str, exp_rate: float = 0.3, lr: float = 0.2, gamma: float = 0.9):\n",
    "        self.name = name\n",
    "        self.states: List[str] = []          # record of state hashes this episode\n",
    "        self.lr = lr                          # learning rate\n",
    "        self.exp_rate = exp_rate              # epsilon\n",
    "        self.gamma = gamma                    # discount factor\n",
    "        self.decay = 0.9995                   # small epsilon decay per move\n",
    "        self.symbol = 1                       # set later by environment\n",
    "        self.states_value = defaultdict(float)  # Q(s)\n",
    "\n",
    "    def choose_action(self, positions: List[Tuple[int, int]], current_board: np.ndarray):\n",
    "        # Îµ-greedy\n",
    "        if np.random.rand() <= self.exp_rate:\n",
    "            idx = np.random.choice(len(positions))\n",
    "            return positions[idx]\n",
    "\n",
    "        # pick action with max Q(s')\n",
    "        value_max = -1e9\n",
    "        action = positions[0]\n",
    "        for p in positions:\n",
    "            next_board = current_board.copy()\n",
    "            next_board[p] = self.symbol\n",
    "            next_hash = str(next_board.reshape(BOARD_ROWS * BOARD_COLS))\n",
    "            value = self.states_value[next_hash]\n",
    "            if value >= value_max:\n",
    "                value_max = value\n",
    "                action = p\n",
    "        return action\n",
    "\n",
    "    def add_state(self, state_hash: str):\n",
    "        self.states.append(state_hash)\n",
    "\n",
    "    def feed_reward(self, reward: float):\n",
    "        # backpropagate through the episode\n",
    "        for st in reversed(self.states):\n",
    "            self.states_value[st] += self.lr * (self.gamma * reward - self.states_value[st])\n",
    "            reward = self.states_value[st]    # bootstrap\n",
    "        self.states = []\n",
    "        # gentle exploration decay over time\n",
    "        self.exp_rate *= self.decay\n",
    "\n",
    "    def reset(self):\n",
    "        self.states = []\n",
    "\n",
    "    def save_policy(self, file: str):\n",
    "        with open(file, \"wb\") as f:\n",
    "            pickle.dump(dict(self.states_value), f)\n",
    "\n",
    "    def load_policy(self, file: str):\n",
    "        with open(file, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        self.states_value.update(data)\n",
    "\n",
    "\n",
    "class HumanPlayer:\n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        self.symbol = -1  # By default, human plays O\n",
    "\n",
    "    def choose_action(self, positions, current_board):\n",
    "        print(\"Available moves:\", positions)\n",
    "        while True:\n",
    "            row = int(input(\"Enter row (0/1/2): \"))\n",
    "            col = int(input(\"Enter col (0/1/2): \"))\n",
    "            if (row, col) in positions:\n",
    "                return (row, col)\n",
    "            print(\"Invalid move, try again.\")\n",
    "\n",
    "    def add_state(self, s): pass\n",
    "    def feed_reward(self, r): pass\n",
    "    def reset(self): pass\n",
    "\n",
    "\n",
    "def train(episodes=50000, save_as: Optional[str] = \"policy_p1.pkl\"):\n",
    "    p1 = Player(\"p1\")\n",
    "    p2 = Player(\"p2\")\n",
    "    st = State(p1, p2)\n",
    "    print(f\"Training for {episodes} self-play games...\")\n",
    "    for ep in range(episodes):\n",
    "        if (ep + 1) % 5000 == 0:\n",
    "            print(\"Episode:\", ep + 1)\n",
    "        while True:\n",
    "            positions = st.available_positions()\n",
    "            p1_action = p1.choose_action(positions, st.board)\n",
    "            st.update_state(p1_action)\n",
    "            st_hash = st.get_hash()\n",
    "            p1.add_state(st_hash)\n",
    "            win = st.winner()\n",
    "            if win is not None:\n",
    "                st.give_reward()\n",
    "                p1.reset(); p2.reset()\n",
    "                st.reset()\n",
    "                break\n",
    "\n",
    "            positions = st.available_positions()\n",
    "            p2_action = p2.choose_action(positions, st.board)\n",
    "            st.update_state(p2_action)\n",
    "            st_hash = st.get_hash()\n",
    "            p2.add_state(st_hash)\n",
    "            win = st.winner()\n",
    "            if win is not None:\n",
    "                st.give_reward()\n",
    "                p1.reset(); p2.reset()\n",
    "                st.reset()\n",
    "                break\n",
    "\n",
    "    if save_as:\n",
    "        p1.save_policy(save_as)\n",
    "        print(f\"Saved Player 1 policy to {save_as}\")\n",
    "    return p1, p2\n",
    "\n",
    "\n",
    "def play_human(policy_file: Optional[str] = \"policy_p1.pkl\"):\n",
    "    # load trained AI as Player 1 (X). Human plays O.\n",
    "    ai = Player(\"AI\", exp_rate=0.0)\n",
    "    if policy_file:\n",
    "        ai.load_policy(policy_file)\n",
    "    human = HumanPlayer(\"You\")\n",
    "    st = State(ai, human)\n",
    "    while True:\n",
    "        # AI move\n",
    "        positions = st.available_positions()\n",
    "        action = ai.choose_action(positions, st.board)\n",
    "        st.update_state(action)\n",
    "        st.show()\n",
    "        win = st.winner()\n",
    "        if win is not None:\n",
    "            if win == 1:\n",
    "                print(\"AI wins!\")\n",
    "            elif win == -1:\n",
    "                print(\"You win!\")\n",
    "            else:\n",
    "                print(\"It's a tie.\")\n",
    "            break\n",
    "\n",
    "        # Human move\n",
    "        positions = st.available_positions()\n",
    "        action = human.choose_action(positions, st.board)\n",
    "        st.update_state(action)\n",
    "        st.show()\n",
    "        win = st.winner()\n",
    "        if win is not None:\n",
    "            if win == 1:\n",
    "                print(\"AI wins!\")\n",
    "            elif win == -1:\n",
    "                print(\"You win!\")\n",
    "            else:\n",
    "                print(\"It's a tie.\")\n",
    "            break\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Train once (or skip if you already have a saved policy)\n",
    "    # train(episodes=50000, save_as=\"policy_p1.pkl\")\n",
    "\n",
    "    # 2) Then play vs the trained AI:\n",
    "    # play_human(\"policy_p1.pkl\")\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (new_env)",
   "language": "python",
   "name": "new_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
